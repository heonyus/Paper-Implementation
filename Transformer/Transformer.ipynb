{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SelfAttention 클래스\n",
    "\n",
    "`SelfAttention` 클래스는 Transformer 아키텍처의 핵심 구성 요소입니다. 주어진 시퀀스에 대한 attention 점수를 계산하고 attention 메커니즘의 주 출력인 값들의 가중치 합을 반환합니다.\n",
    "\n",
    "### 초기화\n",
    "\n",
    "```python\n",
    "def __init__(self, embed_size, heads):\n",
    "```\n",
    "\n",
    "- `embed_size`: 시퀀스의 각 토큰에 대한 임베딩 크기.\n",
    "- `heads`: attention 헤드의 수. 여러 헤드를 사용하면 모델이 입력의 다른 부분에 동시에 집중할 수 있습니다.\n",
    "\n",
    "초기화 내부에서:\n",
    "\n",
    "1. 임베딩 크기는 헤드 수로 나누어 각 헤드의 크기를 결정합니다.\n",
    "2. 값, 키, 쿼리를 위한 선형 계층이 정의됩니다. 이들은 입력 시퀀스를 각각의 표현으로 변환합니다.\n",
    "3. 모든 헤드의 출력을 결합하기 위한 최종 완전 연결(fc) 계층이 정의됩니다.\n",
    "\n",
    "### 순방향 전달\n",
    "\n",
    "```python\n",
    "def forward(self, values, keys, query, mask):\n",
    "```\n",
    "\n",
    "- `values`, `keys`, `query`: 이들은 입력 시퀀스가 각각의 표현으로 변환된 것입니다.\n",
    "- `mask`: 일반적으로 패딩 토큰을 위해 사용되는 값들을 마스킹하는 데 사용됩니다.\n",
    "\n",
    "단계:\n",
    "\n",
    "1. 입력 시퀀스는 헤드를 분리하기 위해 재구성됩니다.\n",
    "2. 시퀀스는 그들의 각각의 선형 계층을 통과합니다.\n",
    "3. 스케일링 된 내적 attention은 다음과 같이 계산됩니다: $\\text{attention} = \\frac{\\text{queries} \\times \\text{keys}^T}{\\sqrt{\\text{head\\_dim}}}$\n",
    "4. 마스크가 제공되면, attention 점수는 마스크에 따라 조정됩니다.\n",
    "5. attention 점수는 softmax 함수를 통해 정규화됩니다.\n",
    "6. 출력은 attention 가중치와 값의 내적으로 계산되며, 모든 헤드의 출력을 결합하기 위해 재구성됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attention = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys]) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = F.softmax(attention, dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerBlock 클래스\n",
    "\n",
    "`TransformerBlock` 클래스는 Transformer 아키텍처의 기본 블록을 나타냅니다. 이 클래스는 Self-Attention 메커니즘과 Feed Forward 신경망을 결합하여 구성됩니다.\n",
    "\n",
    "### 초기화\n",
    "\n",
    "```python\n",
    "def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "```\n",
    "\n",
    "- `embed_size`: 시퀀스의 각 토큰에 대한 임베딩 크기.\n",
    "- `heads`: attention 헤드의 수.\n",
    "- `dropout`: 드롭아웃 비율.\n",
    "- `forward_expansion`: Feed Forward 신경망의 확장 계수.\n",
    "\n",
    "초기화 내부에서:\n",
    "\n",
    "1. `SelfAttention` 메커니즘을 초기화합니다.\n",
    "2. 입력 데이터의 정규화를 위한 두 개의 Layer Normalization 계층을 정의합니다.\n",
    "3. Feed Forward 신경망을 정의합니다. 이 신경망은 두 개의 선형 계층으로 구성되며, 활성화 함수로 ReLU를 사용합니다.\n",
    "4. 드롭아웃 계층을 정의합니다.\n",
    "\n",
    "### 순방향 전달\n",
    "\n",
    "```python\n",
    "def forward(self, value, key, query, mask):\n",
    "```\n",
    "\n",
    "- `value`, `key`, `query`: Self-Attention 메커니즘에 필요한 입력 값입니다.\n",
    "- `mask`: 패딩 토큰을 위한 마스크.\n",
    "\n",
    "단계:\n",
    "\n",
    "1. 입력 값은 `SelfAttention` 메커니즘을 통과하여 attention 값을 얻습니다.\n",
    "2. 얻은 attention 값과 원래의 query 값을 더하여 skip connection을 구현합니다.\n",
    "3. 결과는 첫 번째 Layer Normalization과 드롭아웃을 통과합니다.\n",
    "4. 그 다음, Feed Forward 신경망을 통과합니다.\n",
    "5. Feed Forward 신경망의 출력과 이전의 출력을 더하여 또 다른 skip connection을 구현합니다.\n",
    "6. 마지막으로, 두 번째 Layer Normalization과 드롭아웃을 통과하여 최종 출력을 얻습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "\n",
    "        # Add skip connection, run through normalization and finally dropout\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attention = self.attention(x, x, x, mask)\n",
    "        # Add skip connection, run through normalization and finally dropout\n",
    "        x = self.dropout(self.norm1(attention + x))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder 클래스\n",
    "\n",
    "`Encoder` 클래스는 Transformer 아키텍처의 인코더 부분을 나타냅니다. 이 클래스는 주어진 입력 시퀀스를 처리하여 연속적인 표현을 생성합니다.\n",
    "\n",
    "### 초기화\n",
    "\n",
    "```python\n",
    "def __init__(\n",
    "    self,\n",
    "    src_vocab_size,\n",
    "    embed_size,\n",
    "    num_layers,\n",
    "    heads,\n",
    "    device,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    max_length,\n",
    "):\n",
    "```\n",
    "\n",
    "- `src_vocab_size`: 소스 어휘의 크기.\n",
    "- `embed_size`: 시퀀스의 각 토큰에 대한 임베딩 크기.\n",
    "- `num_layers`: Transformer 블록의 수.\n",
    "- `heads`: attention 헤드의 수.\n",
    "- `device`: 연산을 수행할 장치 (예: \"cuda\" 또는 \"cpu\").\n",
    "- `forward_expansion`: Feed Forward 신경망의 확장 계수.\n",
    "- `dropout`: 드롭아웃 비율.\n",
    "- `max_length`: 입력 시퀀스의 최대 길이.\n",
    "\n",
    "초기화 내부에서:\n",
    "\n",
    "1. 단어 임베딩과 위치 임베딩을 정의합니다.\n",
    "2. 지정된 수의 `TransformerBlock` 레이어를 초기화하여 모듈 리스트에 추가합니다.\n",
    "3. 드롭아웃 계층을 정의합니다.\n",
    "\n",
    "### 순방향 전달\n",
    "\n",
    "```python\n",
    "def forward(self, x, mask):\n",
    "```\n",
    "\n",
    "- `x`: 입력 시퀀스.\n",
    "- `mask`: 패딩 토큰을 위한 마스크.\n",
    "\n",
    "단계:\n",
    "\n",
    "1. 입력 시퀀스의 길이와 배치 크기를 가져옵니다.\n",
    "2. 입력 시퀀스의 각 토큰에 대한 위치를 계산합니다.\n",
    "3. 단어 임베딩과 위치 임베딩을 합하여 최종 임베딩을 얻습니다. 이후 드롭아웃을 적용합니다.\n",
    "4. 최종 임베딩은 모든 `TransformerBlock` 레이어를 순차적으로 통과합니다.\n",
    "5. 최종 출력을 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        device,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_length,\n",
    "    ):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    dropout=dropout,\n",
    "                    forward_expansion=forward_expansion,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        out = self.dropout(\n",
    "            (self.word_embedding(x) + self.position_embedding(positions))\n",
    "        )\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecoderBlock 클래스\n",
    "\n",
    "`DecoderBlock` 클래스는 Transformer 아키텍처의 디코더 부분의 핵심 구성 요소입니다. 이 클래스는 인코더의 출력과 함께 타겟 시퀀스를 처리하여 최종 출력을 생성합니다.\n",
    "\n",
    "### 초기화\n",
    "\n",
    "```python\n",
    "def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "```\n",
    "\n",
    "- `embed_size`: 시퀀스의 각 토큰에 대한 임베딩 크기.\n",
    "- `heads`: attention 헤드의 수.\n",
    "- `forward_expansion`: Feed Forward 신경망의 확장 계수.\n",
    "- `dropout`: 드롭아웃 비율.\n",
    "- `device`: 연산을 수행할 장치 (예: \"cuda\" 또는 \"cpu\").\n",
    "\n",
    "초기화 내부에서:\n",
    "\n",
    "1. 입력 데이터의 정규화를 위한 Layer Normalization 계층을 정의합니다.\n",
    "2. Self-Attention 메커니즘을 초기화합니다.\n",
    "3. `TransformerBlock` 레이어를 초기화합니다.\n",
    "4. 드롭아웃 계층을 정의합니다.\n",
    "\n",
    "### 순방향 전달\n",
    "\n",
    "```python\n",
    "def forward(self, x, value, key, src_mask, trg_mask):\n",
    "```\n",
    "\n",
    "- `x`: 디코더의 입력 시퀀스.\n",
    "- `value`, `key`: 인코더의 출력 값.\n",
    "- `src_mask`: 소스 패딩 토큰을 위한 마스크.\n",
    "- `trg_mask`: 타겟 패딩 토큰을 위한 마스크.\n",
    "\n",
    "단계:\n",
    "\n",
    "1. 입력 `x`는 Self-Attention 메커니즘을 통과하여 attention 값을 얻습니다.\n",
    "2. 얻은 attention 값과 원래의 `x` 값을 더하여 skip connection을 구현합니다. 결과는 Layer Normalization과 드롭아웃을 통과합니다.\n",
    "3. 이후, `TransformerBlock` 레이어를 통과하여 최종 출력을 얻습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            embed_size, heads, dropout, forward_expansion\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        attention = self.attention(x, x, x, trg_mask)\n",
    "        query = self.dropout(self.norm(attention + x))\n",
    "        out = self.transformer_block(value, key, query, src_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        trg_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        device,\n",
    "        max_length,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        trg_pad_idx,\n",
    "        embed_size=512,\n",
    "        num_layers=6,\n",
    "        forward_expansion=4,\n",
    "        heads=8,\n",
    "        dropout=0,\n",
    "        device=\"cuda\",\n",
    "        max_length=100,\n",
    "    ):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            device,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            trg_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        )\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.0003\n",
    "SRC_VOCAB_SIZE = 5000  # Example value, adjust as needed\n",
    "TRG_VOCAB_SIZE = 5000  # Example value, adjust as needed\n",
    "SRC_PAD_IDX = 0\n",
    "TRG_PAD_IDX = 0\n",
    "\n",
    "# Create a random dataset for demonstration purposes\n",
    "# Normally, you'd load your actual dataset here\n",
    "def create_data(n_samples=1000, max_length=50, src_vocab_size=5000, trg_vocab_size=5000):\n",
    "    src_data = torch.randint(0, src_vocab_size - 1, (n_samples, max_length))\n",
    "    trg_data = torch.randint(0, trg_vocab_size - 1, (n_samples, max_length))\n",
    "    return src_data, trg_data\n",
    "\n",
    "src_data, trg_data = create_data()\n",
    "\n",
    "# Initialize the Transformer model\n",
    "model = Transformer(\n",
    "    SRC_VOCAB_SIZE, TRG_VOCAB_SIZE, SRC_PAD_IDX, TRG_PAD_IDX\n",
    ").to(\"cuda\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch_idx in range(0, len(src_data), BATCH_SIZE):\n",
    "        src = src_data[batch_idx: batch_idx + BATCH_SIZE].to(\"cuda\")\n",
    "        trg = trg_data[batch_idx: batch_idx + BATCH_SIZE].to(\"cuda\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg[:, :-1])\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}\")\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = TensorDataset(src_data, trg_data)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Training and validation loop\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, trg in train_loader:\n",
    "        src, trg = src.to(\"cuda\"), trg.to(\"cuda\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg[:, :-1])\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Training Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, trg in val_loader:\n",
    "            src, trg = src.to(\"cuda\"), trg.to(\"cuda\")\n",
    "            output = model(src, trg[:, :-1])\n",
    "            output = output.reshape(-1, output.shape[2])\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Validation Loss: {total_loss / len(val_loader)}\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"transformer_model.pth\")\n",
    "print(\"Model saved!\")\n",
    "\n",
    "# Load the model (if needed later)\n",
    "model.load_state_dict(torch.load(\"transformer_model.pth\"))\n",
    "print(\"Model loaded!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
