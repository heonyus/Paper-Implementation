{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# code by Tae Hwan Jung(Jeff Jung) @graykode\n",
    "# Reference : https://github.com/jadore801120/attention-is-all-you-need-pytorch\n",
    "#           https://github.com/JayParks/transformer, https://github.com/dhlee347/pytorchic-bert\n",
    "import math\n",
    "import re\n",
    "from random import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample IsNext and NotNext to be same in small batch size\n",
    "def make_batch():\n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "    while positive != batch_size/2 or negative != batch_size/2:\n",
    "        tokens_a_index, tokens_b_index= randrange(len(sentences)), randrange(len(sentences)) # 문장 길이 범위내 랜덤 index 설정\n",
    "        tokens_a, tokens_b= token_list[tokens_a_index], token_list[tokens_b_index] # 입력 문장의 token 중 index로 뽑아내기\n",
    "        input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']] # word_dict = PAD + CLS + SEP +  MASK + token\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1) # 0 or1\n",
    "        # token_a : CLS + tokens_a + SEP\n",
    "        # token_b : tokens_b + SEP \n",
    "\n",
    "        # MASK LM\n",
    "        # The training data generator chooses 15% of the token positions at random for prediction\n",
    "        # 훈련 데이터 생성기는 예측을 위해 토큰 위치의 15%를 무작위로 선택합니다\n",
    "        n_pred =  min(max_pred, max(1, int(round(len(input_ids) * 0.15)))) # 전체 중 15%만 맞출거야!\n",
    "        cand_maked_pos = [i for i, token in enumerate(input_ids)\n",
    "                          if token != word_dict['[CLS]'] and token != word_dict['[SEP]']] # input_ids 중 CLS와 SEP token이 아닌 token의 index\n",
    "        shuffle(cand_maked_pos) # shuffle -> 뒤에 랜덤한 15%를 mask하기 위해 사전에 shuffle\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        for pos in cand_maked_pos[:n_pred]: # 15%의 mask 중 -> 사전에 shuffle을 했기 때문에 [:n_pred] = 15% random 추출\n",
    "            masked_pos.append(pos) # masking 할 token의 index\n",
    "            masked_tokens.append(input_ids[pos]) # masking 할 token\n",
    "            if random() < 0.8:  # 80%\n",
    "                input_ids[pos] = word_dict['[MASK]'] # make mask\n",
    "            elif random() < 0.5:  # 10%\n",
    "                index = randint(0, vocab_size - 1) # random index in vocabulary\n",
    "                input_ids[pos] = word_dict[number_dict[index]] # replace\n",
    "\n",
    "        # Zero Paddings -> max_len 사이즈로 zero_padding\n",
    "        n_pad = maxlen - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "\n",
    "        # Zero Padding (100% - 15%) tokens -> max_pred size 만큼 mask_tokens을 zero_padding\n",
    "        if max_pred > n_pred:\n",
    "            n_pad = max_pred - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2: # 위에서 랜덤하게 뽑은 두 문장이 연결되는 문장이고 positive가 batch_size/2라면\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2: # 두 문장이 연결되는 문장이 아니라면\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
    "            negative += 1\n",
    "    return batch\n",
    "# Proprecessing Finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k): # Attention을 구할 때 Padding 부분을 제외하기 위한 Mask\n",
    "                                     # Padding = False, Tokens = True\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
